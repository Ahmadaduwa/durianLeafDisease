{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d6d6cf",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cc9a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Option #py is not recognized. Please run 'nvidia-smi -h'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi #py 3.10.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f80f545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count data\n",
    "import os\n",
    "\n",
    "# Define the paths to the directories\n",
    "train_dir = \"./../../data/train/\"\n",
    "test_dir = \"./../../data/test/\"\n",
    "\n",
    "categories = [\n",
    "    \"ALGAL_LEAF_SPOT\",\n",
    "    \"ALLOCARIDARA_ATTACK\",\n",
    "    \"HEALTHY_LEAF\",\n",
    "    \"LEAF_BLIGHT\",\n",
    "    \"PHOMOPSIS_LEAF_SPOT\"\n",
    "]\n",
    "\n",
    "print(\"--- Training Set File Counts ---\")\n",
    "for category in categories:\n",
    "    dir_path = os.path.join(train_dir, category)\n",
    "    try:\n",
    "        count = len(os.listdir(dir_path))\n",
    "        print(f\"Number of files in {dir_path}: {count}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory not found: {dir_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for {dir_path}: {e}\")\n",
    "\n",
    "print(\"\\n--- Test Set File Counts ---\")\n",
    "for category in categories:\n",
    "    dir_path = os.path.join(test_dir, category)\n",
    "    try:\n",
    "        count = len(os.listdir(dir_path))\n",
    "        print(f\"Number of files in {dir_path}: {count}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory not found: {dir_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for {dir_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f56c842",
   "metadata": {},
   "source": [
    "# Install Timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c09ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Timm (Need to restart the runtime after finish install )\n",
    "#pip install git+https://github.com/rwightman/pytorch-image-models.git\n",
    "#%pip install lightning transformers datasets evaluate pillow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5172a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "\n",
    "# Pytorch Image model (TIMM) library: a library for state-of-the-art image classification\n",
    "import timm\n",
    "import timm.optim\n",
    "import timm.scheduler\n",
    "from timm.data import ImageDataset, create_dataset, create_loader\n",
    "from timm.data.transforms_factory import create_transform\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "\n",
    "from lightning.fabric import Fabric\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "import shutil\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a87861",
   "metadata": {},
   "source": [
    "## visualization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9914fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    %pip install -q torchinfo\n",
    "    from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71075a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model (List of available is shown above)\n",
    "mobilenetv3_large_100 = \"hf_hub:timm/mobilenetv3_large_100.miil_in21k_ft_in1k\"\n",
    "mobilenetv3_large_100_model= timm.create_model(mobilenetv3_large_100, pretrained=True, num_classes=5)\n",
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=mobilenetv3_large_100_model,\n",
    "        input_size=(16, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f63f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform image data based on ImageNet's mean and std\n",
    "data_transforms = { # เปลี่ยนชื่อตัวแปรเป็น data_transforms\n",
    "    \"train\": T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=torch.tensor([0.4850, 0.4560, 0.4060]), std=torch.tensor([0.2290, 0.2240, 0.2250]))\n",
    "    ]),\n",
    "    \"test\": T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=torch.tensor([0.4850, 0.4560, 0.4060]), std=torch.tensor([0.2290, 0.2240, 0.2250]))\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f2f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root=train_dir,\n",
    "                               transform=data_transforms[\"train\"], \n",
    "                               target_transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fed117",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac540021",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f4657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afbe690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select Model\n",
    "model_name = \"hf_hub:timm/mobilenetv3_large_100.miil_in21k_ft_in1k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b28f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Cross Validation Configuration\n",
    "k_splits = 5\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827f3a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "kf = KFold(n_splits=k_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Accumulation Settings\n",
    "# Set to 1 for no accumulation\n",
    "train_batch_size = 16\n",
    "eval_batch_size = 8\n",
    "num_accumulate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42062166",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "#fabric = Fabric(accelerator=\"cuda\", precision=\"16-mixed\")\n",
    "fabric = Fabric(accelerator=\"gpu\", devices=1, precision=\"16-mixed\")\n",
    "fabric.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b291cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_eval_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"Fold {fold+1} of {k_splits}\")\n",
    "\n",
    "    # Load Model\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=5)\n",
    "\n",
    "\n",
    "    # Load Optimizer and Scheduler\n",
    "    optimizer = timm.optim.create_optimizer_v2(model, opt=\"AdamW\", lr=5e-4)\n",
    "    # optimizer = timm.optim.Lookahead(optimizer, alpha=0.5, k=6)    # update the slow weight every k steps\n",
    "                                                                   # update the optimizer by combine slow weight and fast weight * alpha\n",
    "\n",
    "    model, optimizer = fabric.setup(model, optimizer)\n",
    "\n",
    "    scheduler = timm.scheduler.create_scheduler_v2(optimizer, num_epochs=num_epochs)[0]\n",
    "\n",
    "    # Load Data: split train and valition set based on kfold\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=eval_batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    train_dataloader, val_dataloader = fabric.setup_dataloaders(train_dataloader, val_dataloader)\n",
    "\n",
    "    # Reset Model Info\n",
    "    info = {\n",
    "        \"metric_train\": [],\n",
    "        \"metric_val\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"best_metric_val\": -999,\n",
    "        \"best_val_loss\": 999,\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss_epoch = []\n",
    "        val_loss_epoch = []\n",
    "\n",
    "        train_preds = []\n",
    "        train_targets = []\n",
    "\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "\n",
    "        num_updates = epoch * len(train_dataloader)\n",
    "\n",
    "        ### === Train Loop === ###\n",
    "        ## Time\n",
    "        s1 = time.time()\n",
    "\n",
    "        model.train()\n",
    "        for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "            inputs, targets = batch\n",
    "            # inputs = {k: v.to(device) for k,v in inputs.items()}\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            fabric.backward(loss)\n",
    "\n",
    "            # === Gradient Accumulation === #\n",
    "            if ((idx + 1) % num_accumulate == 0) or (idx + 1 == len(train_dataloader)):\n",
    "                optimizer.step()\n",
    "                scheduler.step_update(num_updates=num_updates)\n",
    "                optimizer.zero_grad()\n",
    "            # ============================= #\n",
    "\n",
    "            train_loss_epoch.append(loss.item())\n",
    "            train_preds += outputs.argmax(-1).detach().cpu().tolist()\n",
    "            train_targets += targets.tolist()\n",
    "        ### ==================== ###\n",
    "\n",
    "        # optimizer.sync_lookahead()              # Sync slow weight and fast weight\n",
    "        scheduler.step(epoch + 1)\n",
    "\n",
    "        ### === Evaluation Loop === ###\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader):\n",
    "                inputs, targets = batch\n",
    "                # inputs = {k: v.to(device) for k,v in inputs.items()}\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                # Log Values\n",
    "                val_loss_epoch.append(loss.item())\n",
    "                val_preds += outputs.argmax(-1).detach().cpu().tolist()\n",
    "                val_targets += targets.tolist()\n",
    "        ### ======================= ###\n",
    "        ## Time train finish\n",
    "        s2 = time.time()\n",
    "        elapsed_time = s2 - s1\n",
    "\n",
    "\n",
    "        # Log Data\n",
    "        metric_train = metric.compute(predictions=train_preds, references=train_targets)[\"accuracy\"]\n",
    "        metric_val = metric.compute(predictions=val_preds, references=val_targets)[\"accuracy\"]\n",
    "\n",
    "        info[\"metric_train\"].append(metric_train)\n",
    "        info[\"metric_val\"].append(metric_val)\n",
    "\n",
    "        info[\"train_loss\"].append(np.average(train_loss_epoch))\n",
    "        info[\"val_loss\"].append(np.average(val_loss_epoch))\n",
    "\n",
    "        if metric_val > info[\"best_metric_val\"]:\n",
    "        # if info[\"val_loss\"][-1] < info[\"best_val_loss\"]:\n",
    "            print(\"New Best Score!\")\n",
    "            # print(\"New Best Val Loss\")\n",
    "            info[\"best_metric_val\"] = metric_val\n",
    "            # info[\"best_val_loss\"] = info[\"val_loss\"][-1]\n",
    "            torch.save(model, f\"mobilenetv3_large_100_checkpoint_fold{fold}.pt\")\n",
    "\n",
    "        print(f\"Using time of Fold: {fold} | Epoch: {epoch} | {elapsed_time} second \")\n",
    "        print(info)\n",
    "        print(f\"Fold: {fold} | Epoch: {epoch} | Metric: {metric_val} | Training Loss: {np.average(train_loss_epoch)} | Validation Loss: {np.average(val_loss_epoch)}\")\n",
    "\n",
    "    # save all best metric val\n",
    "    all_eval_scores.append(info[\"best_metric_val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51352d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "classname = dataset.classes\n",
    "classname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0461124",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(k_splits):\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    loaded_model = torch.load(f\"mobilenetv3_large_100_checkpoint_fold{fold}.pt\", weights_only=False)\n",
    "    loaded_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=f\"Evaluating Fold {fold}\"):\n",
    "            inputs, targets = batch\n",
    "            outputs = loaded_model(inputs.to(device))\n",
    "\n",
    "            predictions += outputs.argmax(-1).detach().cpu().tolist()\n",
    "            references += targets.tolist()\n",
    "\n",
    "    print(f\"\\n--- Results for Fold: {fold} ---\")\n",
    "\n",
    "    cm = confusion_matrix(references, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classname)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    disp.plot(ax=ax)\n",
    "\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=8)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=8)\n",
    "\n",
    "    plt.title(f\"Confusion Matrix - Fold {fold}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(references, predictions, target_names=classname))\n",
    "    print(\"-\" * 50) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b045fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "classname = dataset.classes\n",
    "classname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8cd7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Dataset\n",
    "test_dataset = datasets.ImageFolder(root=test_dir,\n",
    "                               transform=data_transforms[\"test\"], # ใช้ data_transforms แทน\n",
    "                               target_transform=None)# transforms to perform on labels (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f1037",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcef65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making test dataloader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=eval_batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656831fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a6b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cdcf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(k_splits):\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    loaded_model = torch.load(f\"mobilenetv3_large_100_checkpoint_fold{fold}.pt\", weights_only=False)\n",
    "    loaded_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=f\"Evaluating Fold {fold}\"):\n",
    "            inputs, targets = batch\n",
    "            outputs = loaded_model(inputs.to(device))\n",
    "\n",
    "            predictions += outputs.argmax(-1).detach().cpu().tolist()\n",
    "            references += targets.tolist()\n",
    "\n",
    "    print(f\"\\n--- Results for Fold: {fold} ---\")\n",
    "\n",
    "    cm = confusion_matrix(references, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classname)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    disp.plot(ax=ax)\n",
    "\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=8)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=8)\n",
    "\n",
    "    plt.title(f\"Confusion Matrix - Fold {fold}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(references, predictions, target_names=classname))\n",
    "    print(\"-\" * 50) #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31e3c8",
   "metadata": {},
   "source": [
    "## Visualization train model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making Pridcition return class & prob\n",
    "from typing import List, Tuple\n",
    "\n",
    "from PIL import Image\n",
    "def pred_class(model: torch.nn.Module,\n",
    "                        image_path: str,\n",
    "                        class_names: List[str],\n",
    "                        image_size: Tuple[int, int] = (224, 224),\n",
    "                        transform: T = None,\n",
    "                        device: torch.device=device):\n",
    "\n",
    "\n",
    "    # 2. Open image\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # 3. Create transformation for image (if one doesn't exist)\n",
    "    if transform is not None:\n",
    "        image_transform = transform\n",
    "    else:\n",
    "        image_transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    ### Predict on image ###\n",
    "\n",
    "    # 4. Make sure the model is on the target device\n",
    "    model.to(device)\n",
    "\n",
    "    # 5. Turn on model evaluation mode and inference mode\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "      # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n",
    "      transformed_image = image_transform(img).unsqueeze(dim=0)\n",
    "\n",
    "      # 7. Make a prediction on image with an extra dimension and send it to the target device\n",
    "      target_image_pred = model(transformed_image.to(device))\n",
    "\n",
    "    # 8. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
    "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
    "\n",
    "    # 9. Convert prediction probabilities -> prediction labels\n",
    "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
    "\n",
    "    classname =  class_names[target_image_pred_label]\n",
    "    prob = target_image_pred_probs.max().cpu().numpy()\n",
    "\n",
    "    return classname , prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c66e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load some model\n",
    "loaded_model = torch.load('mobilenetv3_large_100_checkpoint_fold0.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee55d0",
   "metadata": {},
   "source": [
    "## Train data visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e4bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making df for rando\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# create an empty list to store image paths\n",
    "image_paths = []\n",
    "\n",
    "# loop through each subfolder in the \"Image\" directory\n",
    "for root, dirs, files in os.walk(\"./../../data/test/\"):\n",
    "    for subfolder_name in dirs:\n",
    "        # get the path to the subfolder\n",
    "        subfolder_path = os.path.join(root, subfolder_name)\n",
    "        # loop through each file in the subfolder\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            image_path = os.path.join(subfolder_path, filename)\n",
    "            image_paths.append((image_path, subfolder_name))\n",
    "\n",
    "# create a DataFrame from the list of image paths\n",
    "df = pd.DataFrame(image_paths, columns=['path', 'subfolder_name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6194ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "def sample_picture(df=df,random_state=None):\n",
    "    # Sample 20 random rows from the DataFrame\n",
    "    sample_df = df.sample(20, random_state=random_state).copy()\n",
    "    sample_df = sample_df.reset_index(drop=True)\n",
    "    # Define the grid layout for displaying the images\n",
    "    num_rows = 4\n",
    "    num_cols = 5\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 12))\n",
    "    fig.tight_layout(pad=5.0)\n",
    "\n",
    "    # Iterate over the sampled rows and display the images in the grid\n",
    "    for i, row in sample_df.iterrows():\n",
    "        img = Image.open(row['path'])\n",
    "\n",
    "        # Prediction\n",
    "        pred_name , prob = pred_class(model=loaded_model,image_path=row['path'],\n",
    "                   class_names = classname,\n",
    "                   transform=data_transforms[\"test\"])\n",
    "\n",
    "        row_idx = i // num_cols\n",
    "        col_idx = i % num_cols\n",
    "        axs[row_idx, col_idx].imshow(img)\n",
    "        axs[row_idx, col_idx].axis('on')\n",
    "        axs[row_idx, col_idx].set_title(row['subfolder_name'] + ': \\nPredict:' + pred_name + '\\nProb:'+ str(prob.round(decimals=2)) )\n",
    "\n",
    "        # prediction\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6792f9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_picture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7560f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_picture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_picture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c702c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_picture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b41bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_picture()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
